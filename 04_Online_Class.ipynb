{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327beca8",
   "metadata": {},
   "source": [
    "# Week04 Online Class\n",
    "(21.09.28 12:00 Gitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69923522",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#스파크-모듈-(5개)\" data-toc-modified-id=\"스파크-모듈-(5개)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>스파크 모듈 (5개)</a></span></li><li><span><a href=\"#스파크-아키텍쳐-(4개)\" data-toc-modified-id=\"스파크-아키텍쳐-(4개)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>스파크 아키텍쳐 (4개)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-Driver-:\" data-toc-modified-id=\"Spark-Driver-:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Spark Driver :</a></span></li><li><span><a href=\"#Cluster-Manger-:\" data-toc-modified-id=\"Cluster-Manger-:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Cluster Manger :</a></span></li><li><span><a href=\"#Worker-:\" data-toc-modified-id=\"Worker-:-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Worker :</a></span></li><li><span><a href=\"#Executor-:\" data-toc-modified-id=\"Executor-:-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Executor :</a></span></li><li><span><a href=\"#아키텍쳐가-중요한-이유-:\" data-toc-modified-id=\"아키텍쳐가-중요한-이유-:-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>아키텍쳐가 중요한 이유 :</a></span></li></ul></li><li><span><a href=\"#주피터-노트북에서-\\를-두개-사용하는-이유\" data-toc-modified-id=\"주피터-노트북에서-\\를-두개-사용하는-이유-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>주피터 노트북에서 \\를 두개 사용하는 이유</a></span></li><li><span><a href=\"#Spark-생성\" data-toc-modified-id=\"Spark-생성-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Spark 생성</a></span><ul class=\"toc-item\"><li><span><a href=\"#설정을-반영하여-스파크-설정\" data-toc-modified-id=\"설정을-반영하여-스파크-설정-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>설정을 반영하여 스파크 설정</a></span></li><li><span><a href=\"#Spark-Session-생성-후-설정-변경은-어렵다\" data-toc-modified-id=\"Spark-Session-생성-후-설정-변경은-어렵다-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Spark Session 생성 후 설정 변경은 어렵다</a></span></li></ul></li><li><span><a href=\"#conf-디렉토리-안에는-스파크의-데이터구조가-3개\" data-toc-modified-id=\"conf-디렉토리-안에는-스파크의-데이터구조가-3개-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>conf 디렉토리 안에는 스파크의 데이터구조가 3개</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD\" data-toc-modified-id=\"RDD-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>RDD</a></span></li><li><span><a href=\"#csv\" data-toc-modified-id=\"csv-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>csv</a></span><ul class=\"toc-item\"><li><span><a href=\"#csv파일-읽기\" data-toc-modified-id=\"csv파일-읽기-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>csv파일 읽기</a></span></li><li><span><a href=\"#text파일-읽기\" data-toc-modified-id=\"text파일-읽기-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>text파일 읽기</a></span></li><li><span><a href=\"#한글-읽기\" data-toc-modified-id=\"한글-읽기-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>한글 읽기</a></span></li></ul></li></ul></li><li><span><a href=\"#spark-submit\" data-toc-modified-id=\"spark-submit-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>spark-submit</a></span></li><li><span><a href=\"#lamda\" data-toc-modified-id=\"lamda-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>lamda</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#질문:-pyspark,-spark-submit을-실행하면서-winutils.exe-경고.\" data-toc-modified-id=\"질문:-pyspark,-spark-submit을-실행하면서-winutils.exe-경고.-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>질문: pyspark, spark-submit을 실행하면서 winutils.exe 경고.</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfcade",
   "metadata": {},
   "source": [
    "## 스파크 모듈 (5개)\n",
    "- spark Core, Spark SQL, Spark Streaming, MLlib, GraphX\n",
    "- streaming, graph는 배우지 않음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4e6b0",
   "metadata": {},
   "source": [
    "## 스파크 아키텍쳐 (4개)\n",
    "### Spark Driver :\n",
    "\n",
    "스파크 드라이버는 경로 설정할 때 PYSPARK_DRIVER_PYTHON에 쓰인 Driver\n",
    "\n",
    "### Cluster Manger :\n",
    "`- Standalone 로컬에서 사용하는 경우이고 ```--master local``` 설정.`\n",
    "\n",
    "우리가 사용하는 것\n",
    "`- yarn: Hadoop 2세대에 적용, 대규모 클러스터의 관리자 (```--master yarn``` 이라고 설정)`\n",
    "yarn : 하둡의 아키텍처\n",
    "\n",
    "### Worker : \n",
    "\n",
    "Slave에서 시행, Executor를 가지고 있고 Task 실행\n",
    "### Executor : \n",
    "\n",
    "Taks를 실행한 결과를 Driver에 반환\n",
    "\n",
    "### 아키텍쳐가 중요한 이유 :\n",
    "\n",
    "클러스터로 하니까 여러 컴퓨터들이 협력하도록 하려면 아키텍처가 필요\n",
    "\n",
    "기본적으로 클라이언트-서버 구조\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc2bce",
   "metadata": {},
   "source": [
    "## 주피터 노트북에서 \\를 두개 사용하는 이유\n",
    "- `\\`는 탈출 문자 (escape character)\n",
    "- `\\` 다음에 오는 문제는 실제 문자가 아닌 메타 문자로 인식\n",
    "\n",
    "ex) `\\name` 은 `\\n`으로 인식하여 new line이 된다\n",
    "- 따라서 `\\`를 실제 문자로 인식하려면 `\\\\`로 사용해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4631b",
   "metadata": {},
   "source": [
    "## Spark 생성\n",
    "\n",
    "설정을 넣을 때 비워둠\n",
    "\n",
    "`myConf=pyspark.SparkConf()`\n",
    "\n",
    "필요한 설정을 넣어야 함\n",
    "\n",
    "`myConf=pyspark.SparkConf().set(\"spark.driver.bindAddress\", \"127.0.0.1\")`\n",
    "\n",
    "### 설정을 반영하여 스파크 설정\n",
    "\n",
    "```\n",
    "spark = pyspark.sql.SparkSession\\   # Spark SQL의 시작점\n",
    "    .builder\\                       # SparkSession의 Builder, 객체를 생성함.\n",
    "    .master(\"local\")\\               # (1) 분산의 경우 master URL 또는 (2) 로컬인 경우 ```local[]```라고 적어준다.\n",
    "    .appName(\"myApp\")\\              # app의 명칭\n",
    "    .config(conf=myConf)\\           # 설정을 반영\n",
    "    .getOrCreate()                  # 현재 SparkSession이 있으면 그것을 쓰고, 아니면 생성\n",
    "```\n",
    "\n",
    "### Spark Session 생성 후 설정 변경은 어렵다\n",
    "\n",
    "변경하고 싶다면 sparkContext를 경유하여 `spark.sparkContext._conf.set()` 함수로 변경\n",
    "\n",
    "`spark.sparkContext._conf.set(\"spark.driver.bindAddress\", \"127.0.0.1\")`\n",
    "\n",
    "full-version인 경우 설정변경 template이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b7417",
   "metadata": {},
   "source": [
    "## conf 디렉토리 안에는 스파크의 데이터구조가 3개\n",
    "- RDD\n",
    "- Dataframe\n",
    "- Dataset\n",
    "\n",
    "### RDD\n",
    "\n",
    "파일이라 생각하자\n",
    "\n",
    "- RDD 리스트 생성\n",
    "\n",
    "```\n",
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "```\n",
    "\n",
    "`parallelize()`의 반환은 RDD이다\n",
    "```\n",
    "myRdd1.take(3)\n",
    "```\n",
    "->  `[1, 2, 3]`\n",
    "\n",
    "!주의\n",
    "- RDD가 아니라 list가 반환.\n",
    "- action함수(변환함수x)\n",
    "함수는 (1) 변환, (2) 액션으로 나눌 수 있는데\n",
    "\n",
    "변환 함수는 RDD를 반환 (데이터프레임은 데이터프레임을 반환)\n",
    "\n",
    "-> `parallelize()`은 변환함수라 반환이 RDD이다\n",
    "```\n",
    "spark.sparkContext.parallelize([0, 2, 3, 4, 6], 2).collect()\n",
    "```\n",
    "-> `[[0, 2], [3, 4], [6]]`\n",
    "괄호의 안의 2 : slice, 파티션 나누기\n",
    "\n",
    "파티션은 빅데이터에서 분할하여 동시에 병렬처리함\n",
    "\n",
    "Pandas대신 Spark는 분산, 클러스터 처리를 위해 사용\n",
    "```\n",
    "spark.sparkContext.parallelize([1,2,3,4,5,6,7], 2).glom().collect()\n",
    "```\n",
    "-> `[[1, 2, 3], [4, 5, 6, 7]]`\n",
    "\n",
    "파티션 별로 나눠 출력\n",
    "\n",
    "### csv\n",
    ",로 컬럼을 구분하는 파일\n",
    "#### csv파일 읽기\n",
    "```\n",
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "```\n",
    "#### text파일 읽기\n",
    "```\n",
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "```\n",
    "#### 한글 읽기\n",
    "바이너리로 읽기\n",
    "```\n",
    "popRddBin = spark.sparkContext.binaryFiles(os.path.join(\"data\",\"경기도 의정부시_인구현황_20200904.csv\"))\n",
    "_my = popRddBin.map(lambda x :x[1].decode('euc-kr'))\n",
    "```\n",
    "데이터프레임으로 읽기\n",
    "```\n",
    "popDf = spark\\\n",
    "            .read.option(\"charset\", \"euc-kr\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(os.path.join(\"data\",\"경기도 의정부시_인구현황_20200904.csv\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c36d70",
   "metadata": {},
   "source": [
    "## spark-submit\n",
    "## lamda\n",
    "```\n",
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print (squared)\n",
    "```\n",
    "-> `1, 4 , 9 ,16`\n",
    "\n",
    "`map()`은 변환함수라 RDD가 반환됨 list가 아님\n",
    "\n",
    "`collect()`는 액션함수\n",
    "\n",
    "```\n",
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd4.take(5)\n",
    "```\n",
    "-> `['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']`\n",
    "\n",
    "```\n",
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)\n",
    "```\n",
    "->  `[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]`\n",
    "\n",
    "문자열로 나누어짐\n",
    "- 문자열을 정수로 만들기\n",
    "```\n",
    "myRdd6 = myRdd5.map(lambda x: [int(i) for i in x])\n",
    "myRdd6.take(5)\n",
    "```\n",
    "-> `[[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]]`\n",
    "int도 필요하지만 map이 중요\n",
    "\n",
    "map은 요소별로 적용한다는 의미\n",
    "\n",
    "map은 반복문\n",
    "\n",
    "맵리듀스 알고리즘은 반복문을 없개고 람다 함수로 변환, 출력\n",
    "- 맵-리듀스 알고리즘\n",
    "반복문은 제거한다\n",
    "`myRdd100 = spark.sparkContext.parallelize(range(1,101))`\n",
    "\n",
    "`myRdd100.reduce(lambda subtotal, x: subtotal + x)`\n",
    "\n",
    "`print (\"sum: \", myRdd100.sum())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513be05",
   "metadata": {},
   "source": [
    "#### 질문: pyspark, spark-submit을 실행하면서 winutils.exe 경고.\n",
    "pyspark 실행하면서 이런 WARN. 오류가 아니라서 문제가 당장 되지는 않겠지만, 경고를 제거하려면:\n",
    "```\n",
    "21/09/19 02:18:46 WARN Shell: Did not find winutils.exe: {}\n",
    "java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "```\n",
    "- 윈도우에서는 Hadoop을 직접 내려받아 설치하거나, 'winutils.exe'를 설치해야 한다 (https://github.com/steveloughran/winutils/)\n",
    "- %HADOOP_HOME%\\bin을 만들고 그 아래 'winutils.exe'를 복사 (예를 들어 HADOOP_HOME을 C:\\winutils로 설정하고, 그 안에 C:\\winutils\\bin 만들고 넣음)\n",
    "(강의자료 참조)\n",
    "\n",
    "winutils.exe, hadoop.dll을 설치하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
