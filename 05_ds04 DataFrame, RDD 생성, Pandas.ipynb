{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae68aa5f",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DataFrame-생성\" data-toc-modified-id=\"DataFrame-생성-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DataFrame 생성</a></span><ul class=\"toc-item\"><li><span><a href=\"#자동-schema\" data-toc-modified-id=\"자동-schema-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>자동 schema</a></span></li><li><span><a href=\"#컬럼명-설정\" data-toc-modified-id=\"컬럼명-설정-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>컬럼명 설정</a></span></li><li><span><a href=\"#Row-객체를-사용하여-생성\" data-toc-modified-id=\"Row-객체를-사용하여-생성-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Row 객체를 사용하여 생성</a></span></li></ul></li><li><span><a href=\"#RDD에서-생성하기\" data-toc-modified-id=\"RDD에서-생성하기-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>RDD에서 생성하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#Schema-자동-인식\" data-toc-modified-id=\"Schema-자동-인식-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Schema 자동 인식</a></span></li><li><span><a href=\"#Row를-사용\" data-toc-modified-id=\"Row를-사용-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Row를 사용</a></span></li><li><span><a href=\"#Schema를-정의하고-생성\" data-toc-modified-id=\"Schema를-정의하고-생성-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Schema를 정의하고 생성</a></span></li></ul></li><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataframe을-Pandas로-변환\" data-toc-modified-id=\"Dataframe을-Pandas로-변환-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dataframe을 Pandas로 변환</a></span></li><li><span><a href=\"#Pandas에서-csv쓰기\" data-toc-modified-id=\"Pandas에서-csv쓰기-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Pandas에서 csv쓰기</a></span></li><li><span><a href=\"#Pandas에서-컬럼-생성,-삭제\" data-toc-modified-id=\"Pandas에서-컬럼-생성,-삭제-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Pandas에서 컬럼 생성, 삭제</a></span></li><li><span><a href=\"#csv-파일에서-생성\" data-toc-modified-id=\"csv-파일에서-생성-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>csv 파일에서 생성</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD에서-DataFrame\" data-toc-modified-id=\"RDD에서-DataFrame-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>RDD에서 DataFrame</a></span></li><li><span><a href=\"#DataFrame으로-직접-읽기\" data-toc-modified-id=\"DataFrame으로-직접-읽기-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>DataFrame으로 직접 읽기</a></span></li><li><span><a href=\"#format-load\" data-toc-modified-id=\"format-load-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>format load</a></span></li><li><span><a href=\"#csv\" data-toc-modified-id=\"csv-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>csv</a></span></li></ul></li><li><span><a href=\"#tsv-파일읽기\" data-toc-modified-id=\"tsv-파일읽기-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>tsv 파일읽기</a></span><ul class=\"toc-item\"><li><span><a href=\"#RDD로-읽기\" data-toc-modified-id=\"RDD로-읽기-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>RDD로 읽기</a></span></li><li><span><a href=\"#형변환\" data-toc-modified-id=\"형변환-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>형변환</a></span></li><li><span><a href=\"#Schema-설정\" data-toc-modified-id=\"Schema-설정-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Schema 설정</a></span></li><li><span><a href=\"#DataFrame으로-읽기\" data-toc-modified-id=\"DataFrame으로-읽기-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>DataFrame으로 읽기</a></span></li><li><span><a href=\"#Row의-split-/-csv함수로-tsv-읽기\" data-toc-modified-id=\"Row의-split-/-csv함수로-tsv-읽기-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Row의 split / csv함수로 tsv 읽기</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eff93cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print (spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ab037",
   "metadata": {},
   "source": [
    "## DataFrame 생성\n",
    "schema를 정해주지 않으면 Spark가 자동으로 유추\n",
    "### 자동 schema\n",
    "\n",
    "- 컬럼명은 일련번호를 가지고 생성\n",
    "- 열은 '_1', '_2'와 같이 명명\n",
    "- 데이터 타잎도 유추해서 생성한다. 올바르게 되지 않을 경우가 많다\n",
    "- nullable은 결측값이 허용되는지를 말하는 것이다. true이면 허용된다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c40ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36217425",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1afea8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e331fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7294252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print (myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b0220",
   "metadata": {},
   "source": [
    "### 컬럼명 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd8b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6de5878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year', 'name', 'height']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246e36d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9064f",
   "metadata": {},
   "source": [
    "데이터 100개를 생성하여 names, items를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0091e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a9d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6884927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ada85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026cb6a",
   "metadata": {},
   "source": [
    "### Row 객체를 사용하여 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113ca48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','lee, yw',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc319f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 lee, yw 150\n"
     ]
    }
   ],
   "source": [
    "print (\"row1: \", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9b8f3",
   "metadata": {},
   "source": [
    "- Row를 Dictionary로 저장\n",
    "\n",
    "Row는 속성명과 값을 가지고 있어 쉽게 변환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b339c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '1', 'name': 'lee, yw', 'height': 150}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0290a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['year', 'name', 'height'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00ef9e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['1', 'lee, yw', 150])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba799ed",
   "metadata": {},
   "source": [
    "- Row에서 DataFrame생성\n",
    "\n",
    "Row를 사용하여 DataFrame 만들기\n",
    "\n",
    "Python list에 Row를 넣어 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "964bf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20c81075",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78f5e3",
   "metadata": {},
   "source": [
    "데이터 타입은 자동 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea4b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|lee, yw|   150|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (myDf.printSchema())\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73007a4b",
   "metadata": {},
   "source": [
    "- 데이터타입 정의\n",
    "\n",
    "StructType으로 구조체 선언하고 컬럼에 대해 StructField 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cbf70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbce3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cc52712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eceba31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='1', name='lee, yw', height=150)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c1602",
   "metadata": {},
   "source": [
    "## RDD에서 생성하기\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터, schema를 정의하지 않아도 Spark가 유추함\n",
    "### Schema 자동 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac9f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','lee, yw',150), ('1','lee, sm', 175), ('2','lim, yg',180), ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ac39afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75123a1c",
   "metadata": {},
   "source": [
    "`toDF()`로 변환하거나 직접 `createDataFram()`을 이용하여 DataFrame을 생성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad10059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDf=myRdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21cf757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7e176",
   "metadata": {},
   "source": [
    "RDD에서 또한 DataFrame을 생성할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "381e8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d0651b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d864c82",
   "metadata": {},
   "source": [
    "### Row를 사용\n",
    "RDD의 `map()`을 이용하여 형변환을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e26384fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]), name=x[1], height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccc68a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed6d1692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54c41f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1, name='lee, yw', height=150)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6f42d",
   "metadata": {},
   "source": [
    "`Row`를 사용하여 RDD를 생성할 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad2537cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1=Row(name=\"yw1\", age=10)\n",
    "r2=Row(name=\"yw2\", age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f4c0ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='yw1', age=10), Row(name='yw2', age=20)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8fc8a",
   "metadata": {},
   "source": [
    "### Schema를 정의하고 생성\n",
    "- schema를 정의하고 RDD에서 DataFrame을 생성할 수 있다.\n",
    "- StructType을 선언하고, 컬럼에 대해 StructField를 컬럼명, 데이터 타잎, NULL이 허용되는지 여부를 설정한다. \n",
    "- 과거 버전은 컬럼명이 정렬돼 순서가 변경됐지만 현재는 변경되지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7681669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dcde3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8d6d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| yw1| 10|\n",
      "| yw2| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b60802",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "- Pandas는 데이터 양이적은경우 \n",
    "- Spark DataFrame 양이 많은 경우(빅데이터) 분산처리\n",
    "\n",
    "\n",
    "구분|DataFrame|Pandas\n",
    "----|----|----\n",
    "csv 파일 읽기|read.json()|read_csv()\n",
    "데이터타입|inferschema=True 설정하면 추정|모두 strings\n",
    "\n",
    "### Dataframe을 Pandas로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "834c1fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, yw</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  lee, yw     150\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454bf6f",
   "metadata": {},
   "source": [
    "### Pandas에서 csv쓰기\n",
    "DataFrame을 csv파일로 내보내고 Pandas로 읽기\n",
    "\n",
    "DataFrame to csv 라이브러리 `com.databricks.spark.csv` 사용(파일이 아닌 디렉토리가 생성)\n",
    "\n",
    "한번 생성되면 덮어쓰기를 하지않기 빼문에 새로 쓰고 싶다면 이전 디렉토리를 삭제해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "294f0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0dd0c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 4E88-AB80\n",
      "\n",
      " C:\\Users\\Nunu\\BigData\\data\\_myDf.csv 디렉터리\n",
      "\n",
      "2021-10-17  오후 08:07    <DIR>          .\n",
      "2021-10-17  오후 08:07    <DIR>          ..\n",
      "2021-10-17  오후 08:07                12 .part-00000-88d2bfa2-f1d8-48b6-9962-f4bb738ee717-c000.csv.crc\n",
      "2021-10-17  오후 08:07                 8 ._SUCCESS.crc\n",
      "2021-10-17  오후 08:07                62 part-00000-88d2bfa2-f1d8-48b6-9962-f4bb738ee717-c000.csv\n",
      "2021-10-17  오후 08:07                 0 _SUCCESS\n",
      "               4개 파일                  82 바이트\n",
      "               2개 디렉터리  34,942,214,144 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir data\\_myDf.csv\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd8ef3",
   "metadata": {},
   "source": [
    "Pandas를 이용하여 DataFrame을 csv파일로 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66e46078",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ed61e",
   "metadata": {},
   "source": [
    "### Pandas에서 컬럼 생성, 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2698090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f56a565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81\n",
       "1        Japan     82\n",
       "2    Hong Kong    852"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a62056",
   "metadata": {},
   "source": [
    "전화코드가 81인 경우 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc322ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc['codes']==81]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad4732",
   "metadata": {},
   "source": [
    "### csv 파일에서 생성\n",
    "#### RDD에서 DataFrame\n",
    "`sparkContext.textFile()`로 읽은 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af1ea13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(\"data\", \"ds_spark_2cols.csv\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8f3cf",
   "metadata": {},
   "source": [
    "RDD에서 `Row()`로 Dataframe으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08b9feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "_col12 = lines.map(lambda l: l.split(\",\"))\n",
    "col12 = _col12.map(lambda p: Row(col1=int(p[0].strip()), col2=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(col12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0b4d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col1=35, col2=2),\n",
       " Row(col1=40, col2=27),\n",
       " Row(col1=12, col2=38),\n",
       " Row(col1=15, col2=31),\n",
       " Row(col1=21, col2=1),\n",
       " Row(col1=14, col2=19),\n",
       " Row(col1=46, col2=1),\n",
       " Row(col1=10, col2=34),\n",
       " Row(col1=28, col2=3),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=16, col2=2),\n",
       " Row(col1=30, col2=3),\n",
       " Row(col1=32, col2=2),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=31, col2=2),\n",
       " Row(col1=22, col2=1),\n",
       " Row(col1=12, col2=3),\n",
       " Row(col1=39, col2=29),\n",
       " Row(col1=19, col2=37),\n",
       " Row(col1=25, col2=2)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1801b8",
   "metadata": {},
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "`format().load()` 또는 `csv()` 함수로 csv 파일을 읽어 DataFrame을 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b447a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da6608",
   "metadata": {},
   "source": [
    "#### format load\n",
    "csv 패키지를 사용하여 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a208816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "847bd8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7090628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a91aec1",
   "metadata": {},
   "source": [
    "inferschema를 제외하면, string으로 자동인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e4d4bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- 3: string (nullable = true)\n",
      " |-- 4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b88d8",
   "metadata": {},
   "source": [
    "#### csv\n",
    "csv에서 직접 DataFrame으로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31caa5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .csv(os.path.join('data', 'ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551a5f6",
   "metadata": {},
   "source": [
    "### tsv 파일읽기\n",
    "tsv (tab-separated values) : Tab으로 분리된 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7df27ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.658985, 4.285136])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f821ee",
   "metadata": {},
   "source": [
    "http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights \n",
    "\n",
    "18세 1993년 18세 이하 225,000 건의 키 (in), 몸무게 (lbs) 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "486b68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa6248b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6013cf2",
   "metadata": {},
   "source": [
    "#### RDD로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2126af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "_tRdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "64ba56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "_tRddSplitted = _tRdd.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e983c0",
   "metadata": {},
   "source": [
    "#### 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fad0ba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=_tRdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a43b4",
   "metadata": {},
   "source": [
    "#### Schema 설정\n",
    "자동 설정은 string으로 읽어지니 형변환을 명시적으로 해줘야 함\n",
    "#### DataFrame으로 읽기\n",
    "`text()`로 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8cbba841",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = spark.read.text(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac13c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b82b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(tDftxt['value'], '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f768f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'split(value, \\t, -1)[1]'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_col.getItem(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9edb50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = tDftxt.withColumn('weight', split_col.getItem(1)) # weight\n",
    "tDftxt = tDftxt.withColumn('height', split_col.getItem(2)) # hight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "571dd005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+\n",
      "|          value|weight|height|\n",
      "+---------------+------+------+\n",
      "| 1\t65.78\t112.99| 65.78|112.99|\n",
      "| 2\t71.52\t136.49| 71.52|136.49|\n",
      "| 3\t69.40\t153.03| 69.40|153.03|\n",
      "| 4\t68.22\t142.34| 68.22|142.34|\n",
      "| 5\t67.79\t144.30| 67.79|144.30|\n",
      "| 6\t68.70\t123.30| 68.70|123.30|\n",
      "| 7\t69.80\t141.49| 69.80|141.49|\n",
      "| 8\t70.01\t136.46| 70.01|136.46|\n",
      "| 9\t67.90\t112.37| 67.90|112.37|\n",
      "|10\t66.78\t120.67| 66.78|120.67|\n",
      "|11\t66.49\t127.45| 66.49|127.45|\n",
      "|12\t67.62\t114.14| 67.62|114.14|\n",
      "|13\t68.30\t125.61| 68.30|125.61|\n",
      "|14\t67.12\t122.46| 67.12|122.46|\n",
      "|15\t68.28\t116.09| 68.28|116.09|\n",
      "|16\t71.09\t140.00| 71.09|140.00|\n",
      "|17\t66.46\t129.50| 66.46|129.50|\n",
      "|18\t68.65\t142.97| 68.65|142.97|\n",
      "|19\t71.23\t137.90| 71.23|137.90|\n",
      "|20\t67.13\t124.04| 67.13|124.04|\n",
      "+---------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703ac04",
   "metadata": {},
   "source": [
    "#### Row의 split / csv함수로 tsv 읽기\n",
    "Row 객체는 split할 수 없으므로 dict으로 변환 후 split해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "701a1a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3| 69.4|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79| 144.3|\n",
      "|  6| 68.7| 123.3|\n",
      "|  7| 69.8|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9| 67.9|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13| 68.3|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09| 140.0|\n",
      "| 17|66.46| 129.5|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23| 137.9|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373d8cf",
   "metadata": {},
   "source": [
    "후에 트위터데이터 읽기까지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
