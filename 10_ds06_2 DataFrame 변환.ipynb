{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9098361",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Labeled-Point를-label,-features-컬럼으로-분해\" data-toc-modified-id=\"Labeled-Point를-label,-features-컬럼으로-분해-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Labeled Point를 label, features 컬럼으로 분해</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataFrame-생성\" data-toc-modified-id=\"DataFrame-생성-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>DataFrame 생성</a></span></li><li><span><a href=\"#LabeledPoint에서-DataFrame-생성\" data-toc-modified-id=\"LabeledPoint에서-DataFrame-생성-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>LabeledPoint에서 DataFrame 생성</a></span></li><li><span><a href=\"#mllib.linalg.Vectors로-DataFrame-생성\" data-toc-modified-id=\"mllib.linalg.Vectors로-DataFrame-생성-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>mllib.linalg.Vectors로 DataFrame 생성</a></span></li><li><span><a href=\"#RDD로-DataFrame-생성(spark)\" data-toc-modified-id=\"RDD로-DataFrame-생성(spark)-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>RDD로 DataFrame 생성(spark)</a></span></li></ul></li><li><span><a href=\"#단어-빈도\" data-toc-modified-id=\"단어-빈도-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>단어 빈도</a></span><ul class=\"toc-item\"><li><span><a href=\"#python로-단어-빈도-계산\" data-toc-modified-id=\"python로-단어-빈도-계산-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>python로 단어 빈도 계산</a></span></li><li><span><a href=\"#Spark의-transformer,-estimator\" data-toc-modified-id=\"Spark의-transformer,-estimator-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Spark의 transformer, estimator</a></span></li><li><span><a href=\"#Tokenizer\" data-toc-modified-id=\"Tokenizer-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Tokenizer</a></span></li><li><span><a href=\"#RegTokenizer\" data-toc-modified-id=\"RegTokenizer-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>RegTokenizer</a></span></li><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Stopwords</a></span></li><li><span><a href=\"#CountVectorizer-(빈도-수-계산)\" data-toc-modified-id=\"CountVectorizer-(빈도-수-계산)-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>CountVectorizer (빈도 수 계산)</a></span><ul class=\"toc-item\"><li><span><a href=\"#sklearn로-CountVectorizer\" data-toc-modified-id=\"sklearn로-CountVectorizer-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>sklearn로 CountVectorizer</a></span></li><li><span><a href=\"#spark로-CountVectorizer\" data-toc-modified-id=\"spark로-CountVectorizer-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>spark로 CountVectorizer</a></span></li></ul></li><li><span><a href=\"#TF-IDF-(단어빈도-반전문서빈도)\" data-toc-modified-id=\"TF-IDF-(단어빈도-반전문서빈도)-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>TF-IDF (단어빈도-반전문서빈도)</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF-계산\" data-toc-modified-id=\"TF-IDF-계산-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>TF-IDF 계산</a></span></li><li><span><a href=\"#sklearn로-TF-IDF계산\" data-toc-modified-id=\"sklearn로-TF-IDF계산-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>sklearn로 TF-IDF계산</a></span></li><li><span><a href=\"#Spark로-TF-IDF-구하기\" data-toc-modified-id=\"Spark로-TF-IDF-구하기-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>Spark로 TF-IDF 구하기</a></span></li></ul></li></ul></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#NGram\" data-toc-modified-id=\"NGram-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>NGram</a></span></li><li><span><a href=\"#StringIndexer\" data-toc-modified-id=\"StringIndexer-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>StringIndexer</a></span></li><li><span><a href=\"#연속-데이터의-변환\" data-toc-modified-id=\"연속-데이터의-변환-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>연속 데이터의 변환</a></span></li><li><span><a href=\"#VectorAssembler\" data-toc-modified-id=\"VectorAssembler-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>VectorAssembler</a></span></li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Pipeline</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b562907",
   "metadata": {},
   "source": [
    "# DataFrame 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06300888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 생성\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03dc83e",
   "metadata": {},
   "source": [
    "##  Labeled Point를 label, features 컬럼으로 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04a7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "p = [[1, [1.0, 2.0, 3.0]], [1, [1.1, 2.1, 3.1]], [0, [1.2, 2.2, 3.3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec008279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "features: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"label: {}\\nfeatures: {}\".format(p[0][0], p[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6c6e0",
   "metadata": {},
   "source": [
    "### DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9779763",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbe5719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f07671",
   "metadata": {},
   "source": [
    "### LabeledPoint에서 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758c4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7990fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 생성\n",
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75af19fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf55381",
   "metadata": {},
   "source": [
    "### mllib.linalg.Vectors로 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe66d2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7c5c1",
   "metadata": {},
   "source": [
    "### RDD로 DataFrame 생성(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ad94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.linalg import SparseVector # mllib ok\n",
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83dec4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d3475df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91a71da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9359eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7a55a",
   "metadata": {},
   "source": [
    "## 단어 빈도\n",
    "- 정량 데이터 : 합계, 평균, 표준편차 등으로 통계량 계산, 진답간 차이 분석\n",
    "- 텍스트 : 단어 빈도로 정량화하여 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a270559",
   "metadata": {},
   "source": [
    "### python로 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c91cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496649e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={} # 딕셔너리\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1556f3",
   "metadata": {},
   "source": [
    "`items()` = {key, value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c90fb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\t1\n",
      "I\t1\n",
      "find\t1\n",
      "myself\t1\n",
      "in\t3\n",
      "times\t1\n",
      "of\t6\n",
      "trouble\t1\n",
      "Mother\t1\n",
      "Mary\t1\n",
      "comes\t1\n",
      "to\t1\n",
      "me\t2\n",
      "Speaking\t2\n",
      "words\t3\n",
      "wisdom,\t3\n",
      "let\t3\n",
      "it\t7\n",
      "be\t7\n",
      "And\t1\n",
      "my\t1\n",
      "hour\t1\n",
      "darkness\t1\n",
      "She\t1\n",
      "is\t1\n",
      "standing\t1\n",
      "right\t1\n",
      "front\t1\n",
      "Let\t4\n",
      "Whisper\t1\n"
     ]
    }
   ],
   "source": [
    "# for k,v in d.iteritems():  # python2\n",
    "for k,v in d.items():\n",
    "    print (\"{}\\t{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb3d25",
   "metadata": {},
   "source": [
    "### Spark의 transformer, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be448311",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fda24288",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afcb763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                        |\n",
      "|나 Let it be                          |\n",
      "|너 Let it be                          |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d528a3",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "- corpus : 여러 문장으로 구성된 텍스트 집합\n",
    "- document : 문장으로 구성된 문서 (한 문장도 가능)\n",
    "- vocabularay : 중복이 없는 단어 집합\n",
    "- tokenizer : document를 단어로 분리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c81ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "387441a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf8a9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f40db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent='When I find myself in times of trouble', words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'])\n",
      "Row(sent='Mother Mary comes to me', words=['mother', 'mary', 'comes', 'to', 'me'])\n",
      "Row(sent='Speaking words of wisdom, let it be', words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'])\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17b799",
   "metadata": {},
   "source": [
    "### RegTokenizer\n",
    "정규표현식으로 단어를 분리\n",
    "- \\s : 공백 문자\n",
    "- \\w : 숫자 및 대소문자 [A-Za-z0-9_]\n",
    "- \\* : 0 또는 그 이상, +는 1 또는 그 이상을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28541758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a626392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69009d4f",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "불용어 : 쓸모가 없는 단어 삭제 \n",
    "\n",
    "제공되있는 영어 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0e7c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0825d",
   "metadata": {},
   "source": [
    "한글 불용어 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b334695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89847215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_93c34a0c35f1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9189b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now i'll you'll he'll she'll we'll they'll i'd you'd he'd she'd we'd they'd i'm you're he's she's it's we're they're i've we've you've they've isn't aren't wasn't weren't haven't hasn't hadn't don't doesn't didn't won't wouldn't shan't shouldn't mustn't can't couldn't cannot could here's how's let's ought that's there's what's when's where's who's why's would 나 너 우리 "
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d6dd81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2e0bc",
   "metadata": {},
   "source": [
    "### CountVectorizer (빈도 수 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb9584",
   "metadata": {},
   "source": [
    "#### sklearn로 CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a33e0",
   "metadata": {},
   "source": [
    "1차원으로 만들기, 1차원만 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3584b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c82f50",
   "metadata": {},
   "source": [
    "불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ba31fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ca4fa",
   "metadata": {},
   "source": [
    "word 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57618871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd769c3",
   "metadata": {},
   "source": [
    "단어 아이디 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "281d9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db084f61",
   "metadata": {},
   "source": [
    "벡터 표 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59dea31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e5802",
   "metadata": {},
   "source": [
    "#### spark로 CountVectorizer\n",
    "sklearn과 입출력이 다름(list사용) \n",
    "- 입력 : Tokenizer 후 사용\n",
    "- 출력 : TF(단어 빈도) 출력\n",
    "- minDf : 최소 이하 제거, 단어빈도가 너무 작으면 제거\n",
    "- maxDF : 최대 이상 제거 ,단어빈도가 너무 많으면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61d35af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0474ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1fbeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print (type(cv),type(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "307a98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDf = cvModel.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63ca5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf997a0",
   "metadata": {},
   "source": [
    "(전체 단어의 개수, 값의 컬럼 번호, 값(단어 빈도))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eba21bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|      우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9513e",
   "metadata": {},
   "source": [
    "전체 단어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2399e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let',\n",
       " 'wisdom,',\n",
       " 'words',\n",
       " 'speaking',\n",
       " 'right',\n",
       " 'trouble',\n",
       " 'find',\n",
       " 'hour',\n",
       " 'times',\n",
       " 'darkness',\n",
       " 'mother',\n",
       " 'whisper',\n",
       " 'front',\n",
       " 'mary',\n",
       " 'comes',\n",
       " 'standing']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7499d",
   "metadata": {},
   "source": [
    "### TF-IDF (단어빈도-반전문서빈도)\n",
    "- Tokenizer 후 사용\n",
    "- HashingTF를 사용함 : 고유번호 충돌 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5afff8",
   "metadata": {},
   "source": [
    "#### TF-IDF 계산\n",
    "- TF : 단어 빈도 수\n",
    "- IDF : 문서의 단어 빈도수 (빈도가 높으면 가중치 낮게, 낮으면 높게)\n",
    "- `tf(d,t)` : 단어 t가 문장 d에 있는 빈도 수 \n",
    "- `df` : 문서빈도, 단어가 포함된 문서 수 \n",
    "- `N` : 전체 문서의 수\n",
    "- `idf` : 단어가 포함된 문서의 비율을 반전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15e835ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf: 2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print (\"idf: {}\".format(idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1b8df",
   "metadata": {},
   "source": [
    "#### sklearn로 TF-IDF계산\n",
    "- `max_df=1.0` = 어떤 단어도 무시하지 말라는 의미(생략가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96548119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aeddf860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 14)\t2.791759469228055\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e01d09",
   "metadata": {},
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36afe085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a125b5",
   "metadata": {},
   "source": [
    "각 단어에 대한 idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f507cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79175947, 2.79175947, 2.79175947, 1.40546511, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.38629436, 2.79175947, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.09861229, 2.09861229, 2.79175947])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077bc04",
   "metadata": {},
   "source": [
    "#### Spark로 TF-IDF 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7e05a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32) #  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e109f",
   "metadata": {},
   "source": [
    "`HashingTF`는 `fit()`을 하지 않고 `transform()`한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4af316b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "518280c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------------------------------------------+\n",
      "|nostops                        |hash                                                    |\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "|[find, times, trouble]         |(262144,[64317,91878,152481],[1.0,1.0,1.0])             |\n",
      "|[mother, mary, comes]          |(262144,[24657,63767,245426],[1.0,1.0,1.0])             |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[hour, darkness]               |(262144,[74517,98431],[1.0,1.0])                        |\n",
      "|[standing, right, front]       |(262144,[84798,218360,229166],[1.0,1.0,1.0])            |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[whisper, words, wisdom,, let] |(262144,[151864,173339,175131,188139],[1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.select(\"nostops\", \"hash\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7f124",
   "metadata": {},
   "source": [
    "<strong>TF-IDF</strong>\n",
    "\n",
    "위의 hashTF는 벡터튜플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "722186bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3918c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['find', 'times', 'trouble'], hash=SparseVector(262144, {64317: 1.0, 91878: 1.0, 152481: 1.0}))\n",
      "Row(nostops=['mother', 'mary', 'comes'], hash=SparseVector(262144, {24657: 1.0, 63767: 1.0, 245426: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['hour', 'darkness'], hash=SparseVector(262144, {74517: 1.0, 98431: 1.0}))\n",
      "Row(nostops=['standing', 'right', 'front'], hash=SparseVector(262144, {84798: 1.0, 218360: 1.0, 229166: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af5292",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "(2013, 구글) 단어를 벡터로 변환 방법\n",
    "- 앞서 배운 Bag of Words 모델 : 단어 순서와 문맥 무시\n",
    "- Word2Vec : 단어 서로의 맥락 또는 연관성을 신경망으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98223026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c787e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0713, 0.0644, 0.0301]))\n",
      "Row(w2v=DenseVector([-0.0305, -0.0586, 0.0072]))\n",
      "Row(w2v=DenseVector([-0.0789, -0.0117, 0.0896]))\n"
     ]
    }
   ],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8196076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------------+\n",
      "|word    |vector                                                           |\n",
      "+--------+-----------------------------------------------------------------+\n",
      "|trouble |[-0.1285979002714157,0.12952831387519836,0.09022101759910583]    |\n",
      "|mother  |[0.024350659921765327,-0.09331908077001572,-0.007160484790802002]|\n",
      "|find    |[-0.05298988148570061,-0.11790914833545685,0.07601316273212433]  |\n",
      "|standing|[-0.15491770207881927,0.15503400564193726,0.1488298624753952]    |\n",
      "|wisdom, |[0.01452748291194439,0.039959874004125595,0.06550206989049911]   |\n",
      "|in      |[-0.15514814853668213,0.16155733168125153,0.04123850539326668]   |\n",
      "|myself  |[0.07425080239772797,0.1610012948513031,0.048661258071660995]    |\n",
      "|is      |[0.016040556132793427,0.06754618883132935,-0.15625640749931335]  |\n",
      "|darkness|[0.16111920773983002,-0.07837177813053131,0.0994482934474945]    |\n",
      "|우리    |[0.1293136328458786,0.15445473790168762,0.09370534121990204]     |\n",
      "|front   |[-0.024222591891884804,0.1391872763633728,-0.09881773591041565]  |\n",
      "|it      |[-0.1515401005744934,-0.0285497959703207,0.14664417505264282]    |\n",
      "|너      |[-0.1342955082654953,-0.04920249059796333,0.11127255111932755]   |\n",
      "|she     |[0.035041581839323044,0.06190735101699829,-0.0659969300031662]   |\n",
      "|comes   |[-0.12776364386081696,-0.09263947606086731,0.13315008580684662]  |\n",
      "|i       |[0.015672139823436737,0.06129558011889458,0.02094956859946251]   |\n",
      "|hour    |[0.019265828654170036,0.0190949197858572,0.05620843172073364]    |\n",
      "|to      |[-0.14876788854599,-0.12205152213573456,-0.09346390515565872]    |\n",
      "|speaking|[-0.14317099750041962,0.08514224737882614,0.0015348512679338455] |\n",
      "|mary    |[-0.010376033373177052,-0.09820026904344559,0.01782364584505558] |\n",
      "+--------+-----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87879499",
   "metadata": {},
   "source": [
    "유사한 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a87dcec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|      to|0.7185395359992981|\n",
      "|speaking|0.6933038830757141|\n",
      "|    when|0.6059595346450806|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"times\", 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edce46",
   "metadata": {},
   "source": [
    "## NGram\n",
    "단어가 n개로 구성됐을 때 의미 있는 문장이 있음. n개의 토큰으로 구성된 순열\n",
    "- unigram : 한단어\n",
    "- bigram : 두단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1204dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cbfff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramDf = ngram.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe02d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4041422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dfe97",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "빈도가 제일 높은순서대로 인덱스 값을 주어줌\n",
    "- `setHandleInvalid(\"skip\")` : 없는 레이블 예외일 때 사용\n",
    "- `skip` : 건너뛰기\n",
    "- `keep` : 유지\n",
    "- `error` : 오류\n",
    "- nominal : 명목 변수 (사자, 호랑이)\n",
    "- ordinal : 순서가 있는 변수 (키, low)\n",
    "- interval : 일정한 간격 (150-165, 165-180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3179e",
   "metadata": {},
   "source": [
    "실험 텍스트에는 명목변수가 없어 문장 전체를 인덱스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bde66033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35bd75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ceda1b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      5.0|\n",
      "|Mother Mary comes...|      3.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      1.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      9.0|\n",
      "|        나 Let it be|      7.0|\n",
      "|        너 Let it be|      8.0|\n",
      "|           Let it be|      2.0|\n",
      "|Whisper words of ...|      6.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a099c2",
   "metadata": {},
   "source": [
    "## 연속 데이터의 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad49c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5db157",
   "metadata": {},
   "source": [
    "컬럼 분리를 위해 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "693b9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36a3810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "405987e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08aee5",
   "metadata": {},
   "source": [
    "임계값(threshold)를 정한 후 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6c225783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66b63ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binDf = binarizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cfe07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d236b",
   "metadata": {},
   "source": [
    "3분화 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e218b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a56413e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdDf = discretizer.fit(binDf).transform(binDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ebdec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781437b1",
   "metadata": {},
   "source": [
    "## VectorAssembler\n",
    "기계학습에서 속성값을 여러 컬럼이 가지고 있을 때 하나의 벡터 컬럼으로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa6902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06453717",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaDf = va.transform(qdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff349be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "152824a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7168c",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "명령어를 처리할 때 단계적으로 모델을 합쳐서 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e2da859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8aa3a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") # 토큰화\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") #해싱\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01) # 이분화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d41ab8",
   "metadata": {},
   "source": [
    "위의 3개를 순서대로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69ece10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5fdf27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cb8b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|label|features                                                                             |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|1.0  |(262144,[74920,89530,107107,148981,167694,173558],[1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|0.0  |(262144,[89530,148981],[1.0,1.0])                                                    |\n",
      "|1.0  |(262144,[36803,173558,209078,228158],[1.0,1.0,1.0,1.0])                              |\n",
      "|0.0  |(262144,[132966,198017],[1.0,1.0])                                                   |\n",
      "|0.0  |(262144,[1074,38977,54556,107480,132786,239859,240944],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('label', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d674d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
